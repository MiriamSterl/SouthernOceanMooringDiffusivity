{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e27393",
   "metadata": {},
   "source": [
    "# Computing temperature properties and mixing length for the SR1b section data, considering the full section (also the part that was not sampled by the DIMES mooring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc27e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import h5py\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.io import loadmat\n",
    "import gsw\n",
    "import csaps\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29d265f",
   "metadata": {},
   "source": [
    "## Read in data, compute SA, CT, $\\phi_{1500}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "488c32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% READ IN HYDROGRAPHIC SECTION DATA\n",
    "data = loadmat('data/SR1b_section/sr1b_all.mat')\n",
    "\n",
    "icyc = np.where(data['buf_year'] != 1999)[1] # exclude 1999\n",
    "\n",
    "buf_lat = data['buf_lat'][0,icyc]\n",
    "buf_lon = data['buf_lon'][0,icyc]\n",
    "buf_sal = data['buf_sal'][:,icyc]\n",
    "buf_temp = data['buf_temp'][:,icyc]\n",
    "buf_ptmp = data['buf_ptmp'][:,icyc]\n",
    "buf_press = data['buf_press'].reshape(3000)\n",
    "buf_year = data['buf_year'][:,icyc].reshape(316)\n",
    "\n",
    "f = h5py.File('data/SR1b_section/sr1b_gamma.mat')\n",
    "buf_gamma = f[\"gamma\"][icyc,:].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ba54ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/5664187/miniconda3/envs/carlo/lib/python3.13/site-packages/csaps/_sspumv.py:301: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  u = la.spsolve(a, b)\n"
     ]
    }
   ],
   "source": [
    "# sample points along section\n",
    "start_lon_vals = []\n",
    "end_lon_vals = []\n",
    "start_lat_vals = []\n",
    "end_lat_vals = []\n",
    "for yr in np.unique(buf_year):\n",
    "  idx_yr = np.where(buf_year == yr)[0]\n",
    "  start_lon_vals.append(np.min(buf_lon[idx_yr]))\n",
    "  end_lon_vals.append(np.max(buf_lon[idx_yr]))\n",
    "  start_lat_vals.append(np.max(buf_lat[idx_yr]))\n",
    "  end_lat_vals.append(np.min(buf_lat[idx_yr]))\n",
    "start_lon = np.mean(np.array(start_lon_vals))\n",
    "end_lon = np.mean(np.array(end_lon_vals))\n",
    "start_lat = np.mean(np.array(start_lat_vals))\n",
    "end_lat = np.mean(np.array(end_lat_vals))\n",
    "\n",
    "# Extract unique points along the section\n",
    "nr_points = 30\n",
    "lon_along_section = np.linspace(start_lon, end_lon, nr_points)\n",
    "lat_along_section = np.linspace(start_lat, end_lat, nr_points)\n",
    "\n",
    "# calculate distance from southern end of section\n",
    "dist_along_section = np.zeros(len(lat_along_section))\n",
    "for i in range(len(dist_along_section)):\n",
    "  dist_along_section[i] = gsw.distance(np.mean(lon_along_section)*np.ones(2), np.array([min(lat_along_section), lat_along_section[i]]))[0]\n",
    "\n",
    "# fit smoothing spline through relationship between distance and latitude/longitude along section\n",
    "pp_dist_lat = csaps.csaps(dist_along_section[::-1], lat_along_section[::-1], smooth=0.85)\n",
    "pp_lat_lon = csaps.csaps(lat_along_section[::-1], lon_along_section[::-1], smooth=0.85)\n",
    "# save the spline fit\n",
    "with open('data/pp_dist_lat.pkl', 'wb') as f:\n",
    "    pickle.dump(pp_dist_lat, f)\n",
    "with open('data/pp_lat_lon.pkl', 'wb') as f:\n",
    "    pickle.dump(pp_lat_lon, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5be1770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate absolute salinity and conservative temperature\n",
    "buf_SA = gsw.SA_from_SP(buf_sal, buf_press.reshape(3000,1), buf_lon.reshape(1,316), buf_lat.reshape(1,316)) # absolute salinity\n",
    "buf_CT = gsw.conversions.CT_from_t(buf_SA, buf_temp, buf_press.reshape(3000,1)) # conservative temperature\n",
    "\n",
    "# compute baroclinic streamfunction\n",
    "gpan_buf = -gsw.geo_strf_dyn_height(buf_SA, buf_CT, buf_press, 1500) #geopotential height anomaly / dynamic height anomaly√ü\n",
    "buf_phi1500 = abs(gpan_buf[249,:]) # index 250 is at 500 dbar\n",
    "\n",
    "# calculate distance from southern end of section\n",
    "buf_dist = np.zeros(len(buf_lat))\n",
    "for i in range(len(buf_lat)):\n",
    "  buf_dist[i] = gsw.distance(np.mean(buf_lon)*np.ones(2), np.array([min(buf_lat), buf_lat[i]]))[0]\n",
    "\n",
    "# fit a smoothing spline through relationship\n",
    "pp_dist = csaps.csaps(buf_phi1500[np.argsort(buf_phi1500)[:-np.count_nonzero(np.isnan(buf_phi1500))]],\n",
    "                      buf_dist[np.argsort(buf_phi1500)[:-np.count_nonzero(np.isnan(buf_phi1500))]],\n",
    "                      smooth = 0.85)\n",
    "\n",
    "# apply fitted spline for current data\n",
    "distfromphi = pp_dist(buf_phi1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daebe1a",
   "metadata": {},
   "source": [
    "## Linear interpolation of variables on desired neutral surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a81d26f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_width = 0.02\n",
    "igamma = np.arange(26.5, 28.34, win_width)\n",
    "surf_press = np.zeros((len(igamma),len(buf_lat)))\n",
    "surf_CT = np.zeros((len(igamma),len(buf_lat)))\n",
    "surf_SA = np.zeros((len(igamma),len(buf_lat)))\n",
    "\n",
    "for j in range(len(buf_lat)):\n",
    "    with warnings.catch_warnings(): # this will suppress all warnings in this block\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for i in range(len(igamma)):\n",
    "            icyc = np.where((buf_gamma[:,j] <= igamma[i] + win_width/2) & (buf_gamma[:,j] >= igamma[i] - win_width/2))[0]\n",
    "            surf_press[i,j] = np.nanmean(buf_press[icyc])\n",
    "            surf_CT[i,j] = np.nanmean(buf_CT[icyc,j])\n",
    "            surf_SA[i,j] = np.nanmean(buf_SA[icyc,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39548451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(26.487517651803728), np.float64(28.340706695653807))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmin(buf_gamma), np.nanmax(buf_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d6c27",
   "metadata": {},
   "source": [
    "## Calculate mean and rms of temperature and salinity plus their gradients on each neutral surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "308dded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit splines on isoneutral surfaces to determine mean relationships\n",
    "pp_CT = [[] for i in range(len(igamma))]  # initialize array to fill with spline fits\n",
    "pp_SA = [[] for i in range(len(igamma))]\n",
    "\n",
    "for j in range(len(igamma)):  # only rows where there are actually non-NaNs, see surf_CT[77,:][~np.isnan(surf_CT[77,:])]\n",
    "    # fit a smoothing spline through relationship\n",
    "    idx_x = np.argsort(distfromphi)[:-np.count_nonzero(np.isnan(distfromphi))] # non-nan indices in x (for spline fit)\n",
    "    idx_y = np.argwhere(~np.isnan(surf_CT[j,:])) #non-nan indices in y      \n",
    "\n",
    "    new_idx = []\n",
    "    for i in range(len(idx_x)):\n",
    "        if idx_x[i] in idx_y:\n",
    "            new_idx.append(idx_x[i])\n",
    "    new_idx = np.asarray(new_idx)\n",
    "\n",
    "    # now do spline fit\n",
    "    smooth = 1e-14\n",
    "    pp_CT[j] = csaps.csaps(distfromphi[new_idx], surf_CT[j,:][new_idx], smooth=smooth)\n",
    "    pp_SA[j] = csaps.csaps(distfromphi[new_idx], surf_SA[j,:][new_idx], smooth=smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c81a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate \"background\" meridional theta gradient and rms theta anomaly for each isopycnal using a running meridional window\n",
    "\n",
    "# define a meridional distance grid, and the width of the running window\n",
    "idist = np.arange(0,680e3,10e3)\n",
    "win_width = 40e3\n",
    "\n",
    "pressmean     = np.zeros((len(igamma), len(idist)))\n",
    "SAbar         = np.zeros((len(igamma), len(idist)))\n",
    "thetabar      = np.zeros((len(igamma), len(idist)))\n",
    "dthetabardy   = np.zeros((len(igamma), len(idist)))\n",
    "rms_theta     = np.zeros((len(igamma), len(idist)))\n",
    "\n",
    "for j in range(len(igamma)):\n",
    "    dthetabardy[j,:] = np.gradient(pp_CT[j](idist),idist)\n",
    "    with warnings.catch_warnings(): # this will suppress all warnings in this block\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for i in range(len(idist)):\n",
    "            icyc = np.where((distfromphi <= idist[i] + win_width/2) & (distfromphi >= idist[i] - win_width/2))[0]\n",
    "\n",
    "            pressmean[j,i] = np.nanmean(surf_press[j,icyc])\n",
    "            thetabar[j,i] = pp_CT[j](idist[i])\n",
    "            SAbar[j,i]    = pp_SA[j](idist[i])\n",
    "\n",
    "            if len(icyc) > 1:\n",
    "                rms_theta[j,i] = np.nanstd(surf_CT[j,icyc] - pp_CT[j](distfromphi[icyc]))\n",
    "            else:\n",
    "                rms_theta[j,i] = np.nan\n",
    "\n",
    "SAbar[np.isnan(pressmean)] = np.nan\n",
    "thetabar[np.isnan(pressmean)] = np.nan\n",
    "dthetabardy[np.isnan(pressmean)] = np.nan\n",
    "rms_theta[np.isnan(pressmean)] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05165a92",
   "metadata": {},
   "source": [
    "## Calculate mixing length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ce20135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixing efficiency parameter\n",
    "gamma_mix = 0.35\n",
    "\n",
    "# based on temperature\n",
    "dthetabardy_limited = dthetabardy.copy()\n",
    "dthetabardy_limited[np.where(abs(dthetabardy_limited) < 1e-7)] = 1e-7\n",
    "Lmix = rms_theta / np.abs(dthetabardy_limited) \n",
    "Lmix[np.where(Lmix == 0)] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099899e6",
   "metadata": {},
   "source": [
    "## Saving the output as a function of cross-stream distance and neutral density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset({\"pressmean\": ([\"gamma\", \"dist\"], pressmean),\n",
    "                 \"thetabar\": ([\"gamma\", \"dist\"], thetabar),\n",
    "                 \"dthetabardy\": ([\"gamma\", \"dist\"], dthetabardy_limited),\n",
    "                 \"rms_theta\": ([\"gamma\", \"dist\"], rms_theta),\n",
    "                 \"SAbar\": ([\"gamma\", \"dist\"], SAbar),\n",
    "                 \"Lmix\": ([\"gamma\", \"dist\"], Lmix)},\n",
    "                coords={\"gamma\": igamma, \"dist\": idist})\n",
    "ds.to_netcdf('data/variables_from_section-dist_gamma-full.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a19ad",
   "metadata": {},
   "source": [
    "## Now transform to more intuitive coordinates: latitude and depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceac9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform cross-stream distance to latitude\n",
    "ilat = pp_dist_lat(idist)  # latitude values corresponding to idist\n",
    "\n",
    "# compute depth from pressure, for both moorings and section data\n",
    "zmean = np.zeros(np.shape(pressmean))\n",
    "for i in range(np.shape(pressmean)[1]):\n",
    "    zmean[:,i] = gsw.z_from_p(pressmean[:,i], pp_dist_lat(idist[i]))*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3cacdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate data from (Y, gamma) to (Y, depth) coordinates (gamma-levels to z-levels)\n",
    "def convert_to_zlevs(data):\n",
    "    z_levels = np.linspace(np.nanmin(zmean), np.nanmax(zmean), 60)\n",
    "    Z = len(z_levels)\n",
    "    L = len(idist)\n",
    "\n",
    "    data_on_press = np.full((Z, L), np.nan)\n",
    "    for i in range(L):\n",
    "        z_col = zmean[:,i]\n",
    "        d_col = data[:,i]\n",
    "\n",
    "        # mask NaNs to avoid interpolation errors\n",
    "        valid = np.isfinite(z_col) & np.isfinite(d_col)\n",
    "        if np.count_nonzero(valid) < 2:\n",
    "            continue  # Not enough points to interpolate\n",
    "\n",
    "        # if pressure is not monotonic, sort it\n",
    "        z_sorted = z_col[valid]\n",
    "        d_sorted = d_col[valid]\n",
    "        sort_idx = np.argsort(z_sorted)\n",
    "        z_sorted = z_sorted[sort_idx]\n",
    "        d_sorted = d_sorted[sort_idx]\n",
    "\n",
    "        # interpolate to the pressure levels\n",
    "        data_on_press[:, i] = np.interp(z_levels, z_sorted, d_sorted, left=np.nan, right=np.nan)\n",
    "    return data_on_press, z_levels\n",
    "\n",
    "# convert variables to z levels\n",
    "thetabar_z, iz = convert_to_zlevs(thetabar)\n",
    "dthetabardy_z, _ = convert_to_zlevs(dthetabardy)\n",
    "dthetabardy_limited_z, _ = convert_to_zlevs(dthetabardy_limited)\n",
    "rms_theta_z, _ = convert_to_zlevs(rms_theta)\n",
    "SAbar_z, _ = convert_to_zlevs(SAbar)\n",
    "Lmix_z, _ = convert_to_zlevs(Lmix)\n",
    "\n",
    "dthetabardy_limited_z[np.where(abs(dthetabardy_limited_z) < 1e-7)] = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1616d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset({\"thetabar\": ([\"depth\", \"lat\"], thetabar_z),\n",
    "                 \"dthetabardy\": ([\"depth\", \"lat\"], dthetabardy_limited_z),\n",
    "                 \"rms_theta\": ([\"depth\", \"lat\"], rms_theta_z),\n",
    "                 \"SAbar\": ([\"depth\", \"lat\"], SAbar_z),\n",
    "                 \"Lmix\": ([\"depth\", \"lat\"], Lmix_z)},\n",
    "                coords={\"depth\": iz, \"lat\": ilat})\n",
    "ds.to_netcdf('data/variables_from_section-lat_depth-full.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42963769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carlo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

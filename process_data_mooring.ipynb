{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69ee496",
   "metadata": {},
   "source": [
    "# Computing temperature properties and mixing length for the DIMES C mooring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237af25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.io import loadmat\n",
    "import gsw\n",
    "import csaps\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8198f4aa",
   "metadata": {},
   "source": [
    "## Define which data set to use for the relation between $\\phi_{1500}$ and the cross-stream distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "183b44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = 'SR1b' # can be 'SR1b', 'CMEMS', 'SOSE', or 'Argo'\n",
    "\n",
    "if data_source == 'SR1b':\n",
    "    fname = 'data/SR1b_section/pp_dist_sr1b.pkl'\n",
    "elif data_source == 'CMEMS':\n",
    "    fname = 'data/CMEMS_reanalysis/pp_dist_cmems.pkl'\n",
    "elif data_source == 'SOSE':\n",
    "    fname = 'data/SOSE_reanalysis/pp_dist_sose.pkl'\n",
    "elif data_source == 'Argo':\n",
    "    fname = 'data/Argo_data/pp_dist_argo.pkl'\n",
    "else:\n",
    "    raise ValueError(\"Invalid data source. Choose from 'SR1b', 'CMEMS', 'SOSE', or 'Argo'.\")\n",
    "\n",
    "with open(fname, 'rb') as f:\n",
    "    pp_dist_func = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd224f",
   "metadata": {},
   "source": [
    "## Read in & preprocess mooring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b435694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in mooring data\n",
    "mooring_c = loadmat('data/DIMES_moorings/mooring_c.mat')['mooring_c']\n",
    "pressure = mooring_c['P'][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81426759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling to daily values\n",
    "\n",
    "def downsample(input_array, factor):\n",
    "    # factor says how much to downsample, i.e. to go from quarter hourly to days we downsample by 24*4=96\n",
    "    shape = input_array.shape\n",
    "    output = np.zeros((shape[0], shape[1]//factor))\n",
    "    for i in range(shape[0]):\n",
    "        output[i,:] = np.nanmean(input_array[i,:].reshape(-1, factor), 1)\n",
    "        \n",
    "    return output\n",
    "\n",
    "downsampling_factor = 96          # 24 * 4, quarterly measurements\n",
    "\n",
    "with warnings.catch_warnings(): # this will suppress all warnings in this block\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    mooring_c['CT'][0][0] = downsample(mooring_c['CT'][0][0], downsampling_factor)\n",
    "    mooring_c['SA'][0][0] = downsample(mooring_c['SA'][0][0], downsampling_factor)\n",
    "    mooring_c['gamma'][0][0] = downsample(mooring_c['gamma'][0][0], downsampling_factor)\n",
    "    mooring_c['U'][0][0] = downsample(mooring_c['U'][0][0].T, downsampling_factor)\n",
    "    mooring_c['V'][0][0] = downsample(mooring_c['V'][0][0].T, downsampling_factor)\n",
    "     \n",
    "length_of_array = mooring_c['CT'][0][0].shape[1]\n",
    "\n",
    "# Conversion of dates from MATLAB format to readable format using pandas\n",
    "# The value 719529 is the datenum value of the Unix epoch start (1970-01-01), which is the default origin for pd.to_datetime().\n",
    "dates_c = pd.to_datetime(mooring_c['DATES'][0][0].reshape(42048) - 719529, unit='D')[::downsampling_factor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1946a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "CT = mooring_c['CT'][0][0]\n",
    "SA = mooring_c['SA'][0][0]\n",
    "gamma = mooring_c['gamma'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9f5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate U and V for all moorings (they are only measured at a few pressure levels)\n",
    "\n",
    "preslevels_c = np.array([453.2, 504.4, 555.1, 602.4, 1233.9, 1332.7, 1888.9, 1987.5, 2085.9, 2190.6, 3434.9, 3637.6])\n",
    "preslevels_others = np.array([453.2, 550, 1200, 2000, 3500])\n",
    "\n",
    "U_c = gsw.pchip_interp(preslevels_c, mooring_c['U'][0][0].T, pressure, axis=1).T\n",
    "V_c = gsw.pchip_interp(preslevels_c, mooring_c['V'][0][0].T, pressure, axis=1).T\n",
    "\n",
    "U = U_c\n",
    "V = V_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b36d150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate baroclinic streamfunctions and use spatial data to deduce distance from southern edge of section\n",
    "\n",
    "gpan_c  = -gsw.geo_strf_dyn_height(mooring_c['SA'][0][0], mooring_c['CT'][0][0], mooring_c['P'][0][0][0])\n",
    "phi1500_c  = gpan_c[105] - gpan_c[5]\n",
    "phi1500 = phi1500_c  # for now only use center mooring, because it has the most data\n",
    "distfromphi = pp_dist_func(phi1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a95c82f",
   "metadata": {},
   "source": [
    "## Linear interpolation of variables on desired neutral surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf8ad076",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_width = 0.02\n",
    "igamma = np.arange(27.14, 28.21, win_width)\n",
    "surf_press = np.zeros((len(igamma),len(distfromphi)))\n",
    "surf_CT = np.zeros((len(igamma),len(distfromphi)))\n",
    "surf_SA = np.zeros((len(igamma),len(distfromphi)))\n",
    "surf_U = np.zeros((len(igamma),len(distfromphi)))\n",
    "surf_V = np.zeros((len(igamma),len(distfromphi)))\n",
    "\n",
    "for j in range(len(distfromphi)):\n",
    "    with warnings.catch_warnings(): # this will suppress all warnings in this block\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for i in range(len(igamma)):\n",
    "            icyc = np.where((gamma[:,j] <= igamma[i] + win_width/2) & (gamma[:,j] >= igamma[i] - win_width/2))[0]\n",
    "            surf_press[i,j] = np.nanmean(pressure[icyc])\n",
    "            surf_CT[i,j] = np.nanmean(CT[icyc,j])\n",
    "            surf_SA[i,j] = np.nanmean(SA[icyc,j])\n",
    "            surf_U[i,j] = np.nanmean(U[icyc,j])\n",
    "            surf_V[i,j] = np.nanmean(V[icyc,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476c297",
   "metadata": {},
   "source": [
    "## Calculate mean and rms of temperature and salinity plus their gradients on each neutral surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd4963b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/5664187/miniconda3/envs/carlo/lib/python3.13/site-packages/csaps/_sspumv.py:301: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  u = la.spsolve(a, b)\n"
     ]
    }
   ],
   "source": [
    "# fit splines on isoneutral surfaces to determine mean relationships\n",
    "pp_CT = [[] for i in range(len(igamma))]  # initialize array to fill with spline fits\n",
    "pp_SA = [[] for i in range(len(igamma))]\n",
    "\n",
    "for j in range(len(igamma)):  # only rows where there are actually non-NaNs, see surf_CT[77,:][~np.isnan(surf_CT[77,:])]\n",
    "    # fit a smoothing spline through relationship\n",
    "    idx_x = np.argsort(distfromphi)#[:-np.count_nonzero(np.isnan(distfromphi))]\n",
    "    idx_y = np.argwhere(~np.isnan(surf_CT[j,:])) #non-nan indices in y   \n",
    "\n",
    "    new_idx = []\n",
    "    for i in range(len(idx_x)):\n",
    "        if idx_x[i] in idx_y:\n",
    "            new_idx.append(idx_x[i])\n",
    "    new_idx = np.asarray(new_idx)\n",
    "\n",
    "    # now do spline fit\n",
    "    smooth = 1e-14\n",
    "    pp_CT[j] = csaps.csaps(distfromphi[new_idx], surf_CT[j,:][new_idx], smooth=smooth)\n",
    "    pp_SA[j] = csaps.csaps(distfromphi[new_idx], surf_SA[j,:][new_idx], smooth=smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c249538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate \"background\" meridional theta gradient and rms theta anomaly for each isopycnal using a running meridional window\n",
    "\n",
    "#   Define a meridional distance grid, and the width of the running window\n",
    "idist     = np.arange(390e3,680e3,10e3)\n",
    "win_width = 40e3\n",
    "\n",
    "pressmean     = np.zeros((len(igamma), len(idist)))\n",
    "SAbar         = np.zeros((len(igamma), len(idist)))\n",
    "thetabar      = np.zeros((len(igamma), len(idist)))\n",
    "dthetabardy   = np.zeros((len(igamma), len(idist)))\n",
    "rms_theta     = np.zeros((len(igamma), len(idist)))\n",
    "\n",
    "\n",
    "for j in range(len(igamma)):\n",
    "    dthetabardy[j,:] = np.gradient(pp_CT[j](idist),idist)\n",
    "    with warnings.catch_warnings(): # this will suppress all warnings in this block\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for i in range(len(idist)):\n",
    "            icyc = np.where((distfromphi <= idist[i] + win_width/2) & (distfromphi >= idist[i] - win_width/2))[0]\n",
    "\n",
    "            pressmean[j,i] = np.nanmean(surf_press[j,icyc])\n",
    "            thetabar[j,i] = pp_CT[j](idist[i])\n",
    "            SAbar[j,i]    = pp_SA[j](idist[i])\n",
    "            \n",
    "            if len(icyc) > 1:\n",
    "                rms_theta[j,i] = np.nanstd(surf_CT[j,icyc] - pp_CT[j](distfromphi[icyc]))\n",
    "            else:\n",
    "                rms_theta[j,i] = np.nan\n",
    "\n",
    "SAbar[np.isnan(pressmean)] = np.nan\n",
    "thetabar[np.isnan(pressmean)] = np.nan\n",
    "dthetabardy[np.isnan(pressmean)] = np.nan\n",
    "rms_theta[np.isnan(pressmean)] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa87d43",
   "metadata": {},
   "source": [
    "## Calculate eddy velocity scale on each neutral surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01e76efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56955/1991924759.py:8: RuntimeWarning: Mean of empty slice\n",
      "  vel_mean = np.nanmean(vel_vec,axis=1) # time-mean velocity vector\n",
      "/nethome/5664187/miniconda3/envs/carlo/lib/python3.13/site-packages/numpy/lib/_nanfunctions_impl.py:2015: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_56955/1991924759.py:15: RuntimeWarning: Mean of empty slice\n",
      "  upar[j,i] = np.nanmean(upari)\n"
     ]
    }
   ],
   "source": [
    "ue = np.zeros((len(igamma), len(idist))) # eddy velocity scale\n",
    "upar = np.zeros((len(igamma), len(idist))) # along-stream velocity\n",
    "\n",
    "for j in range(len(igamma)):\n",
    "    for i in range(len(idist)):\n",
    "        icyc = np.where((distfromphi <= idist[i] + win_width/2) & (distfromphi >= idist[i] - win_width/2))[0]\n",
    "        vel_vec = np.array([surf_U[j,icyc],surf_V[j,icyc]])\n",
    "        vel_mean = np.nanmean(vel_vec,axis=1) # time-mean velocity vector\n",
    "        nv = np.array([-vel_mean[1],vel_mean[0]]) # perpendicular vector to time-mean velocity\n",
    "        n = nv / np.linalg.norm(nv) # normalize\n",
    "        ucross = surf_U[j,icyc]*n[0] + surf_V[j,icyc]*n[1] # compute u . n for all data points in this window\n",
    "        ue[j,i] = np.nanstd(ucross)\n",
    "        p = vel_mean / np.linalg.norm(vel_mean) # normalize vector parallell to time-mean velocity\n",
    "        upari = surf_U[j,icyc]*p[0] + surf_V[j,icyc]*p[1]\n",
    "        upar[j,i] = np.nanmean(upari)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e4764",
   "metadata": {},
   "source": [
    "## Calculate mixing length and eddy diffusivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e579877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixing efficiency parameter\n",
    "gamma_mix = 0.35\n",
    "\n",
    "# based on temperature\n",
    "dthetabardy_limited = dthetabardy.copy()\n",
    "dthetabardy_limited[np.where(abs(dthetabardy_limited) < 1e-7)] = 1e-7\n",
    "Lmix = rms_theta / np.abs(dthetabardy_limited) \n",
    "Lmix[np.where(Lmix == 0)] = np.nan\n",
    "\n",
    "gamma_mix = 0.35 # mixing efficiency parameter\n",
    "eddy_diff = gamma_mix * ue * Lmix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb3db1",
   "metadata": {},
   "source": [
    "## Saving the output as a function of cross-stream distance and neutral density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "291a82c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset({\"pressmean\": ([\"gamma\", \"dist\"], pressmean),\n",
    "                 \"thetabar\": ([\"gamma\", \"dist\"], thetabar),\n",
    "                 \"dthetabardy\": ([\"gamma\", \"dist\"], dthetabardy_limited),\n",
    "                 \"rms_theta\": ([\"gamma\", \"dist\"], rms_theta),\n",
    "                 \"SAbar\": ([\"gamma\", \"dist\"], SAbar),\n",
    "                 \"ue\": ([\"gamma\", \"dist\"], ue),\n",
    "                 \"upar\": ([\"gamma\", \"dist\"], upar),\n",
    "                 \"Lmix\": ([\"gamma\", \"dist\"], Lmix),\n",
    "                 \"eddy_diff\": ([\"gamma\", \"dist\"], eddy_diff)},\n",
    "                coords={\"gamma\": igamma, \"dist\": idist})\n",
    "ds.to_netcdf('data/variables_from_moorings_'+data_source+'-dist_gamma.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf9dfe",
   "metadata": {},
   "source": [
    "## Now transform to more intuitive coordinates: latitude and depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e10fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform cross-stream distance to latitude\n",
    "with open('data/pp_dist_lat.pkl', 'rb') as f:\n",
    "    pp_dist_lat = pickle.load(f)\n",
    "ilat = pp_dist_lat(idist)  # latitude values corresponding to idist\n",
    "\n",
    "# compute depth from pressure, for both moorings and section data\n",
    "zmean = np.zeros(np.shape(pressmean))\n",
    "for i in range(np.shape(pressmean)[1]):\n",
    "    zmean[:,i] = gsw.z_from_p(pressmean[:,i], ilat[i])*-1\n",
    "\n",
    "ds_section_depth = xr.open_dataset('data/variables_from_section-lat_depth.nc')\n",
    "depth_section = ds_section_depth.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6863cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate data from (Y, gamma) to (Y, depth) coordinates (gamma-levels to z-levels)\n",
    "def convert_to_zlevs(data):\n",
    "    z_levels = depth_section.values  # use the depth from the section data\n",
    "    Z = len(z_levels)\n",
    "    L = len(idist)\n",
    "\n",
    "    data_on_press = np.full((Z, L), np.nan)\n",
    "    for i in range(L):\n",
    "        z_col = zmean[:,i]\n",
    "        d_col = data[:,i]\n",
    "\n",
    "        # mask NaNs to avoid interpolation errors\n",
    "        valid = np.isfinite(z_col) & np.isfinite(d_col)\n",
    "        if np.count_nonzero(valid) < 2:\n",
    "            continue  # Not enough points to interpolate\n",
    "\n",
    "        # if pressure is not monotonic, sort it\n",
    "        z_sorted = z_col[valid]\n",
    "        d_sorted = d_col[valid]\n",
    "        sort_idx = np.argsort(z_sorted)\n",
    "        z_sorted = z_sorted[sort_idx]\n",
    "        d_sorted = d_sorted[sort_idx]\n",
    "\n",
    "        # interpolate to the pressure levels\n",
    "        data_on_press[:, i] = np.interp(z_levels, z_sorted, d_sorted, left=np.nan, right=np.nan)\n",
    "    return data_on_press, z_levels\n",
    "\n",
    "# convert variables to z levels\n",
    "thetabar_z, iz = convert_to_zlevs(thetabar)\n",
    "dthetabardy_limited_z, _ = convert_to_zlevs(dthetabardy_limited)\n",
    "rms_theta_z, _ = convert_to_zlevs(rms_theta)\n",
    "SAbar_z, _ = convert_to_zlevs(SAbar)\n",
    "ue_z, _ = convert_to_zlevs(ue)\n",
    "upar_z, _ = convert_to_zlevs(upar)\n",
    "Lmix_z, _ = convert_to_zlevs(Lmix)\n",
    "eddy_diff_z, _ = convert_to_zlevs(eddy_diff)\n",
    "\n",
    "dthetabardy_limited_z[np.where(abs(dthetabardy_limited_z) < 1e-7)] = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9709297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset({\"thetabar\": ([\"depth\", \"lat\"], thetabar_z),\n",
    "                 \"dthetabardy\": ([\"depth\", \"lat\"], dthetabardy_limited_z),\n",
    "                 \"rms_theta\": ([\"depth\", \"lat\"], rms_theta_z),\n",
    "                 \"SAbar\": ([\"depth\", \"lat\"], SAbar_z),\n",
    "                 \"ue\": ([\"depth\", \"lat\"], ue_z),\n",
    "                 \"upar\": ([\"depth\", \"lat\"], upar_z),\n",
    "                 \"Lmix\": ([\"depth\", \"lat\"], Lmix_z),\n",
    "                 \"eddy_diff\": ([\"depth\", \"lat\"], eddy_diff_z)},\n",
    "                coords={\"depth\": iz, \"lat\": ilat})\n",
    "ds.to_netcdf('data/variables_from_moorings_'+data_source+'-lat_depth.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e319a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carlo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
